{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bYvleycK9-h"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download NLTK stopwords and tokenizer if not already present\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Sample text data\n",
        "data = {\n",
        "    'Text': [\n",
        "        \"This is an example of text data.\",\n",
        "        \"We are learning data preprocessing techniques.\",\n",
        "        \"Removing stopwords is an important step.\",\n",
        "        \"Let's clean this sentence by removing the stopwords.\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Define English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function to remove stopwords\n",
        "def remove_stopwords(text):\n",
        "    words = word_tokenize(text.lower())  # lowercase and tokenize\n",
        "    filtered = [word for word in words if word.isalnum() and word not in stop_words]\n",
        "    return ' '.join(filtered)\n",
        "\n",
        "# Apply the function\n",
        "df['Cleaned_Text'] = df['Text'].apply(remove_stopwords)\n",
        "\n",
        "# Display results\n",
        "print(\"Original and Cleaned Text:\")\n",
        "print(df)"
      ]
    }
  ]
}