{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cwaXndjJ8k3"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Sample noisy text data\n",
        "data = {\n",
        "    'Text': [\n",
        "        \"Helloooo!!! This is sooo cooolllll!!!\",\n",
        "        \"Thissss isss a teestt of theee cleeanerrr.\",\n",
        "        \"U r gr8!!! Plz call ASAP!!!\",\n",
        "        \"H@ppy B!rthd@y to uuuu\",\n",
        "        \"L0L!!! I can't stoppp laughingggg!!!\"\n",
        "    ]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Download NLTK resources if not already available\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Initialize\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Cleaning function\n",
        "def clean_text(text):\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "    # Remove digits and special characters\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    # Remove repeated characters (e.g., \"coool\" -> \"cool\")\n",
        "    text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove stopwords and stem\n",
        "    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
        "    # Join back\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Apply cleaning\n",
        "df['Cleaned_Text'] = df['Text'].apply(clean_text)\n",
        "\n",
        "# Display results\n",
        "print(\"Original and Cleaned Text:\")\n",
        "print(df)"
      ]
    }
  ]
}