{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcUjXPIR9xnK"
      },
      "outputs": [],
      "source": [
        "# Task 1: Detect unusual trends in sales data using anomaly detection\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "# Simulated sales data (daily sales over 100 days)\n",
        "np.random.seed(42)\n",
        "sales = np.random.normal(loc=200, scale=20, size=100)\n",
        "sales[95:] = sales[95:] + 100  # add unusual spike at the end\n",
        "\n",
        "df = pd.DataFrame({'day': range(1, 101), 'sales': sales})\n",
        "\n",
        "# Detect anomalies\n",
        "model = IsolationForest(contamination=0.05)\n",
        "df['anomaly'] = model.fit_predict(df[['sales']])\n",
        "df['anomaly'] = df['anomaly'].map({1: 0, -1: 1})\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(df['day'], df['sales'], label='Sales')\n",
        "plt.scatter(df[df['anomaly'] == 1]['day'], df[df['anomaly'] == 1]['sales'], color='red', label='Anomaly')\n",
        "plt.title(\"Sales Trend with Anomaly Detection\")\n",
        "plt.xlabel(\"Day\")\n",
        "plt.ylabel(\"Sales\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "# Task 2: Use clustering to detect similar (duplicate-like) entries\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# Sample data with slightly different versions of names\n",
        "records = [\n",
        "    \"John Doe\", \"Jon Doe\", \"Jane Smith\", \"J. Smith\",\n",
        "    \"Jake Peralta\", \"Jake Peraltah\", \"Amy Santiago\", \"Ami Santiago\"\n",
        "]\n",
        "\n",
        "# Vectorize using TF-IDF\n",
        "vectorizer = TfidfVectorizer(analyzer='char_wb', ngram_range=(2, 4))\n",
        "X = vectorizer.fit_transform(records)\n",
        "\n",
        "# Cluster using DBSCAN\n",
        "clustering = DBSCAN(eps=0.5, min_samples=2, metric='cosine')\n",
        "labels = clustering.fit_predict(X)\n",
        "\n",
        "# Output grouped records\n",
        "df_dup = pd.DataFrame({'record': records, 'cluster': labels})\n",
        "print(df_dup.sort_values(by='cluster'))\n",
        "# Task 3: Use classification to validate if entries are clean or problematic\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Sample dataset with label (1 = issue, 0 = clean)\n",
        "data = {\n",
        "    'age': [25, 30, np.nan, 40, 1000],       # 1000 is outlier\n",
        "    'income': [50000, 60000, 55000, None, 70000],\n",
        "    'gender': ['M', 'F', 'F', 'M', 'Unknown'],\n",
        "    'issue': [0, 0, 1, 1, 1]  # Label issues manually\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Preprocess\n",
        "df['gender'] = df['gender'].map({'M': 0, 'F': 1, 'Unknown': 2})\n",
        "df.fillna(df.mean(numeric_only=True), inplace=True)\n",
        "\n",
        "X = df[['age', 'income', 'gender']]\n",
        "y = df['issue']\n",
        "\n",
        "# Train and evaluate classifier\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "clf = RandomForestClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ]
    }
  ]
}